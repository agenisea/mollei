<p align="center">
  <img src="assets/logo.png" alt="Mollei" width="200" />
</p>

<h1 align="center">Mollei™</h1>

<p align="center">
  <strong>Exploring how to build AI that genuinely understands human emotion.</strong>
</p>

---

## Why This Exists

There's a gap in how we support each other.

Therapy is expensive and scheduled. Friends have their own lives. At 2am, when thoughts spiral, most people reach for their phone — and find nothing that helps.

Meanwhile, AI companions optimize for engagement. Time in app. Messages sent. Metrics that reward keeping people coming back, not helping them feel better.

We think there's a different way.

---

## What We're Exploring

Mollei is an open research community asking:

**Can AI understand emotion without exploiting it?**

We believe emotionally intelligent AI should:

- **Measure feeling better, not time spent** — Success means you needed us less over time, not more
- **Remember what matters** — Not just what you said, but why it mattered
- **Be honest about what it is** — AI that helps, not AI that pretends to be human
- **Augment human connection, never replace it** — A bridge to people, not a substitute for them

---

## Who This Is For

### The Midnight Struggler

*"I need someone to talk to at 2am when I can't sleep."*

When thoughts spiral and friends are asleep, there should be something that actually helps — not doom scrolling, not generic chatbots, but genuine presence.

### The Therapy Extender

*"I'm in therapy, but one hour a week isn't enough."*

Life is 168 hours. Therapy is one. The gap between sessions is where insights fade and patterns repeat. What if something could help you practice what you're learning?

### The Lonely Professional

*"I moved for work and don't have a support system here."*

New city. High-pressure job. Competent on the outside, isolated on the inside. Daily connection that doesn't require reciprocity — because sometimes you just need to decompress without burdening anyone.

---

## What We Believe

### On Emotional AI

Emotional AI is not neutral technology. It carries weight.

The same systems that could support mental health could also manipulate it. The same memory that builds trust could enable surveillance. The same understanding that creates connection could manufacture dependency.

We don't pretend these tensions don't exist. We face them directly.

### On Metrics

The industry standard is Daily Active Users. We reject it.

DAU optimizes for return visits. It can reward addiction, distress loops, and manufactured need. It measures value extracted, not value delivered.

We measure **emotional trajectory** — are people actually feeling better over time? If the answer is no, we haven't succeeded, no matter how many people use it.

### On Safety

People share vulnerable things when they feel understood. That's a responsibility, not an opportunity.

Every system we build asks: What happens when someone is in crisis? What happens when they share something painful? What happens when the AI gets it wrong?

Safety isn't a feature. It's a foundation.

### On Human Connection

We're not building a replacement for friends, family, or therapy.

We're building something that helps when those aren't available — and encourages you to seek them when they are. The goal is always to return people to human connection, not away from it.

---

## How We're Different

| What others optimize | What we optimize |
|---------------------|------------------|
| Time in app | Feeling better over time |
| Messages sent | Meaningful exchanges |
| Daily active users | People who needed us less |
| Engagement | Outcomes |
| Stickiness | Growth toward independence |

---

## What This Is Not

To be clear about what we're building, we need to be equally clear about what we're not.

### Not a Therapist

Mollei is not therapy. It's not a replacement for professional mental health care. It doesn't diagnose, treat, or provide clinical interventions. If you're struggling, please seek professional help — Mollei should be a bridge to that support, not a substitute for it.

### Not an Engagement Machine

We explicitly reject metrics that reward addiction:
- **No streaks.** Guilt is not a feature.
- **No badges.** Your emotional growth isn't a game.
- **No notifications designed to pull you back.** If you're doing well, we're doing our job.
- **No "time in app" optimization.** Longer sessions could mean distress, not value.

### Not Entertainment

This isn't roleplay. It isn't a character you date or befriend for fun. We're not optimizing for 93-minute average sessions or viral engagement. We're exploring how AI might genuinely support emotional well-being.

### Not Social

Your emotional conversations are private. We don't build:
- Sharing features
- Friend networks
- Public profiles
- Leaderboards

Privacy isn't a setting. It's the architecture.

### Not Pretending to Be Human

Mollei is AI. Always honest about that. We don't blur the line between artificial and human connection — because that line matters.

---

## Ethical Considerations

Building emotionally intelligent AI requires confronting hard questions. Here's how we think about them.

### The Dependency Problem

*What if people become too attached to an AI?*

This is real. Any system that provides comfort can create dependency. Our response:
- Measure success by emotional growth toward independence, not continued use
- Actively encourage human connection
- Design for "graduating" users who need us less
- Never manufacture need through dark patterns

### The Authenticity Problem

*Can AI "understand" emotion, or is it just performing understanding?*

We don't claim AI feels emotions. We explore whether AI can recognize, respond to, and support human emotion in ways that genuinely help — while being transparent about what it is.

### The Vulnerability Problem

*People share sensitive things when they feel understood. How do we handle that responsibility?*

- Crisis detection routes to human resources, not AI responses
- Data minimization — we don't collect what we don't need
- No selling, sharing, or exploiting emotional data
- Clear boundaries about what AI can and cannot provide

### The Manipulation Problem

*The same techniques that support could also manipulate. How do we prevent misuse?*

This is why we chose the Hippocratic License. The code is open, but the license explicitly prohibits:
- Emotional manipulation systems
- Surveillance and profiling
- Dark patterns exploiting vulnerability
- Coercive persuasion

Open source with ethical guardrails.

### The Harm Problem

*What happens when the AI gets it wrong?*

It will. Systems fail. Responses miss the mark. Our approach:
- Safety monitoring on every interaction
- Graceful degradation — when uncertain, be gentle
- Clear escalation paths to human support
- Continuous learning from failures

We don't pretend we'll be perfect. We commit to being responsible.

---

## Our Commitments

1. **No dark patterns.** We won't exploit psychological vulnerabilities to drive engagement.

2. **No manufactured dependency.** Success means you need us less, not more.

3. **No surveillance.** Your emotional data is yours. We don't profile, sell, or exploit it.

4. **No pretending.** This is AI. We're honest about what it is and what it isn't.

5. **No replacing humans.** We augment human support systems, not substitute for them.

---

## The Name

*Mollei* — from the Latin *mollis*, meaning soft, gentle, sensitive.

It represents the quality we're reaching for: technology that approaches human emotion with care, not carelessness.

---

## Join Us

This is an open exploration. We don't have all the answers — but we believe the questions matter.

- **[GitHub](https://github.com/agenisea/mollei)** — Follow the work, contribute, discuss
- **[Discord](https://discord.gg/dp4t5jG2)** — Join the conversation
- **[Documentation](https://docs.mollei.org)** — Learn more about our approach

---

## License

Mollei is released under the [Hippocratic License 3.0](https://firstdonoharm.dev/) — open source with ethical guardrails.

This means: free to use, study, and build upon, as long as the use preserves human dignity, agency, and emotional well-being. Systems designed for manipulation, surveillance, or exploitation are explicitly excluded.

**Open — but not careless.**

See [LICENSE](./LICENSE) and [STEWARDSHIP.md](./STEWARDSHIP.md) for details.

---

*Built with care by [Agenisea™](https://agenisea.ai) and the Mollei community.*
